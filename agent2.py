import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from strands import Agent
from strands.models import BedrockModel
import re
import xml.etree.ElementTree as ET

# === Step 1: Claude (via Bedrock) è¨­å®š ===
model = BedrockModel(
    model_id="anthropic.claude-3-5-sonnet-20240620-v1:0",
    anthropic_version="bedrock-2023-05-31"
)
agent = Agent(model=model)

# === Step 2: AWS RSS Feed ã‹ã‚‰1é€±é–“åˆ†å–å¾— ===
def scrape_aws_news_rss():
    """AWS What's Newã®RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰è¨˜äº‹ã‚’å–å¾—"""
    rss_url = "https://aws.amazon.com/new/feed/"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    try:
        res = requests.get(rss_url, headers=headers, timeout=10)
        res.raise_for_status()
    except requests.RequestException as e:
        print(f"ã‚¨ãƒ©ãƒ¼: RSSãƒ•ã‚£ãƒ¼ãƒ‰ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ - {e}")
        return ""
    
    try:
        root = ET.fromstring(res.content)
    except ET.ParseError as e:
        print(f"ã‚¨ãƒ©ãƒ¼: XMLã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸ - {e}")
        return ""
    
    today = datetime.now()
    one_week_ago = today - timedelta(days=7)
    news = []
    
    print(f"æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼: {one_week_ago.strftime('%Y-%m-%d')} ä»¥é™ã®è¨˜äº‹ã‚’æŠ½å‡º")
    
    # RSS itemsã‚’å–å¾—
    items = root.findall('.//item')
    print(f"RSSã‹ã‚‰ {len(items)} ä»¶ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’ç™ºè¦‹")
    
    for i, item in enumerate(items):
        title_elem = item.find('title')
        pub_date_elem = item.find('pubDate')
        desc_elem = item.find('description')
        
        if title_elem is None:
            continue
            
        title = title_elem.text.strip() if title_elem.text else ""
        
        # æ—¥ä»˜ã®è§£æ
        parsed_date = None
        if pub_date_elem is not None and pub_date_elem.text:
            pub_date_text = pub_date_elem.text.strip()
            print(f"ã‚¢ã‚¤ãƒ†ãƒ  {i}: æ—¥ä»˜ãƒ†ã‚­ã‚¹ãƒˆ = '{pub_date_text}'")
            
            # RFC 2822 å½¢å¼ã®æ—¥ä»˜è§£æ (ä¾‹: Thu, 06 Jun 2024 10:00:00 +0000)
            try:
                parsed_date = datetime.strptime(pub_date_text, "%a, %d %b %Y %H:%M:%S %z")
                parsed_date = parsed_date.replace(tzinfo=None)  # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³æƒ…å ±ã‚’å‰Šé™¤
            except ValueError:
                try:
                    parsed_date = datetime.strptime(pub_date_text, "%a, %d %b %Y %H:%M:%S GMT")
                except ValueError:
                    print(f"ã‚¢ã‚¤ãƒ†ãƒ  {i}: æ—¥ä»˜ã®è§£æã«å¤±æ•— '{pub_date_text}'")
                    continue
        else:
            print(f"ã‚¢ã‚¤ãƒ†ãƒ  {i}: æ—¥ä»˜æƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“")
            continue
        
        print(f"ã‚¢ã‚¤ãƒ†ãƒ  {i}: è§£æã•ã‚ŒãŸæ—¥ä»˜ = {parsed_date.strftime('%Y-%m-%d')}")
        
        # 1é€±é–“ä»¥å†…ã‹ãƒã‚§ãƒƒã‚¯
        if parsed_date < one_week_ago:
            print(f"ã‚¢ã‚¤ãƒ†ãƒ  {i}: 1é€±é–“ã‚ˆã‚Šå¤ã„è¨˜äº‹ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
            continue
        
        # èª¬æ˜æ–‡ã®å–å¾—
        desc = ""
        if desc_elem is not None and desc_elem.text:
            desc = desc_elem.text.strip()
            # HTMLã‚¿ã‚°ã‚’å‰Šé™¤
            desc = re.sub(r'<[^>]+>', '', desc)
            # é•·ã„èª¬æ˜æ–‡ã¯çŸ­ç¸®
            if len(desc) > 200:
                desc = desc[:200] + "..."
        
        news_item = f"- [{parsed_date.strftime('%Y-%m-%d')}] {title}"
        if desc:
            news_item += f": {desc}"
        
        news.append(news_item)
        print(f"ã‚¢ã‚¤ãƒ†ãƒ  {i}: è¿½åŠ ã—ã¾ã—ãŸ - {title}")
    
    print(f"\nåˆè¨ˆ {len(news)} ä»¶ã®1é€±é–“ä»¥å†…ã®è¨˜äº‹ã‚’å–å¾—ã—ã¾ã—ãŸ")
    return "\n".join(news)

# === Step 2: AWS News Blog ã‹ã‚‰1é€±é–“åˆ†å–å¾— (ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯) ===
def scrape_aws_news():
    # AWS What's Newãƒšãƒ¼ã‚¸ã¯å‹•çš„èª­ã¿è¾¼ã¿ã®ãŸã‚ã€AWS News Blogã‚’ä½¿ç”¨
    url = "https://aws.amazon.com/blogs/aws/"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    try:
        res = requests.get(url, headers=headers, timeout=10)
        res.raise_for_status()
    except requests.RequestException as e:
        print(f"ã‚¨ãƒ©ãƒ¼: ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ - {e}")
        return ""
    
    soup = BeautifulSoup(res.text, "html.parser")
    
    # AWS News Blogã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚’è©¦ã™
    possible_selectors = [
        "article",
        ".post",
        ".blog-post",
        ".entry",
        ".post-item",
        ".blog-item"
    ]
    
    items = []
    for selector in possible_selectors:
        items = soup.select(selector)
        if items:
            print(f"ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ '{selector}' ã§ {len(items)} ä»¶ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’ç™ºè¦‹")
            break
    
    if not items:
        print("ã‚¢ã‚¤ãƒ†ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚HTMLæ§‹é€ ã‚’ç¢ºèªã—ã¾ã™...")
        # ãƒ‡ãƒãƒƒã‚°ç”¨: å®Ÿéš›ã®HTMLæ§‹é€ ã‚’å°‘ã—è¡¨ç¤º
        print("HTMLæ§‹é€ ã®ã‚µãƒ³ãƒ—ãƒ«:")
        print(soup.prettify()[:2000])
        return ""

    today = datetime.now()
    one_week_ago = today - timedelta(days=7)
    news = []
    
    print(f"æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼: {one_week_ago.strftime('%Y-%m-%d')} ä»¥é™ã®è¨˜äº‹ã‚’æŠ½å‡º")

    for i, item in enumerate(items[:20]):  # æœ€åˆã®20ä»¶ã‚’ãƒã‚§ãƒƒã‚¯
        # æ—¥ä»˜ã®å–å¾—ã‚’è¤‡æ•°ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§è©¦è¡Œ
        date_element = None
        date_selectors = [
            "time",
            ".date",
            ".publish-date",
            ".entry-date",
            ".post-date",
            ".meta-date",
            "[datetime]"
        ]
        
        for date_sel in date_selectors:
            date_element = item.select_one(date_sel)
            if date_element:
                break
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã®å–å¾—
        title_element = None
        title_selectors = [
            "h1",
            "h2",
            "h3",
            ".title",
            ".entry-title",
            ".post-title",
            "a[href*='/blogs/']"
        ]
        
        for title_sel in title_selectors:
            title_element = item.select_one(title_sel)
            if title_element and title_element.text.strip():
                break
        
        if not title_element:
            print(f"ã‚¢ã‚¤ãƒ†ãƒ  {i}: ã‚¿ã‚¤ãƒˆãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            continue

        # æ—¥ä»˜ã®å‡¦ç†
        parsed_date = None
        if date_element:
            # datetimeå±æ€§ãŒã‚ã‚‹å ´åˆ
            if date_element.get('datetime'):
                date_text = date_element.get('datetime')
            else:
                date_text = date_element.text.strip()
            
            print(f"ã‚¢ã‚¤ãƒ†ãƒ  {i}: æ—¥ä»˜ãƒ†ã‚­ã‚¹ãƒˆ = '{date_text}'")
            
            # æ—¥ä»˜è§£æã®è¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³å¯¾å¿œ
            date_patterns = [
                "%Y-%m-%dT%H:%M:%S%z",    # ISO format with timezone
                "%Y-%m-%dT%H:%M:%S",      # ISO format
                "%Y-%m-%d",               # 2024-06-11
                "%B %d, %Y",              # June 11, 2024
                "%d %B %Y",               # 11 June 2024
                "%m/%d/%Y",               # 06/11/2024
                "%Y.%m.%d",               # 2024.06.11
            ]
            
            # æ—¥ä»˜ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ•°å­—éƒ¨åˆ†ã‚’æŠ½å‡º
            date_match = re.search(r'(\d{4})[/-.](\d{1,2})[/-.](\d{1,2})', date_text)
            if date_match:
                year, month, day = date_match.groups()
                try:
                    parsed_date = datetime(int(year), int(month), int(day))
                except ValueError:
                    pass
            
            if not parsed_date:
                # ä»–ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚‚è©¦è¡Œ
                for pattern in date_patterns:
                    try:
                        parsed_date = datetime.strptime(date_text.split('T')[0], pattern.split('T')[0])
                        break
                    except ValueError:
                        continue
        
        # æ—¥ä»˜ãŒå–å¾—ã§ããªã„å ´åˆã¯ã€è¨˜äº‹ã®é †åºã‹ã‚‰æœ€è¿‘ã®ã‚‚ã®ã¨ä»®å®š
        if not parsed_date:
            if i < 5:  # æœ€åˆã®5ä»¶ã¯æœ€è¿‘ã®è¨˜äº‹ã¨ä»®å®š
                parsed_date = today
                print(f"ã‚¢ã‚¤ãƒ†ãƒ  {i}: æ—¥ä»˜ä¸æ˜ã®ãŸã‚æœ€æ–°è¨˜äº‹ã¨ä»®å®š")
            else:
                print(f"ã‚¢ã‚¤ãƒ†ãƒ  {i}: æ—¥ä»˜ã®è§£æã«å¤±æ•—ã€ã‚¹ã‚­ãƒƒãƒ—")
                continue
        else:
            print(f"ã‚¢ã‚¤ãƒ†ãƒ  {i}: è§£æã•ã‚ŒãŸæ—¥ä»˜ = {parsed_date.strftime('%Y-%m-%d')}")
        
        # 1é€±é–“ä»¥å†…ã‹ãƒã‚§ãƒƒã‚¯
        if parsed_date < one_week_ago:
            print(f"ã‚¢ã‚¤ãƒ†ãƒ  {i}: 1é€±é–“ã‚ˆã‚Šå¤ã„è¨˜äº‹ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
            continue

        title = title_element.text.strip()
        
        # èª¬æ˜æ–‡ã®å–å¾—
        desc_element = None
        desc_selectors = [
            ".excerpt",
            ".summary",
            ".description",
            ".post-excerpt",
            ".entry-summary",
            "p"
        ]
        
        for desc_sel in desc_selectors:
            desc_element = item.select_one(desc_sel)
            if desc_element and desc_element.text.strip():
                break
        
        desc = desc_element.text.strip() if desc_element else ""
        # é•·ã„èª¬æ˜æ–‡ã¯çŸ­ç¸®
        if len(desc) > 200:
            desc = desc[:200] + "..."
        
        news_item = f"- [{parsed_date.strftime('%Y-%m-%d')}] {title}"
        if desc:
            news_item += f": {desc}"
        
        news.append(news_item)
        print(f"ã‚¢ã‚¤ãƒ†ãƒ  {i}: è¿½åŠ ã—ã¾ã—ãŸ - {title}")

    print(f"\nåˆè¨ˆ {len(news)} ä»¶ã®1é€±é–“ä»¥å†…ã®è¨˜äº‹ã‚’å–å¾—ã—ã¾ã—ãŸ")
    return "\n".join(news)

# === Step 3: Claudeã«è¦ç´„ã•ã›ã‚‹ ===
def summarize_with_claude(news_text):
    if not news_text.strip():
        return "ã“ã®1é€±é–“ã€AIã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã«é–¢é€£ã™ã‚‹æ–°ã—ã„AWSã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
    
    prompt = f"""
ä»¥ä¸‹ã¯AWSå…¬å¼ã®1é€±é–“åˆ†ã®æ–°ç€æƒ…å ±ã§ã™ã€‚
ã‚ãªãŸã¯AIã«èˆˆå‘³ãŒã‚ã‚‹ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã§ã™ã€‚
ä»¥ä¸‹ã®æƒ…å ±ã‹ã‚‰ã€ã‚ãªãŸã«ã¨ã£ã¦é–¢ä¿‚ãŒæ·±ã„ã‚‚ã®ã ã‘ã‚’é¸ã³ã€è¦ç´„ã—ã¦ãã ã•ã„ï¼š

{news_text}

---
å‡ºåŠ›å½¢å¼ï¼š
1. é–¢é€£ãŒã‚ã‚‹ãƒˆãƒ”ãƒƒã‚¯å
2. ãªãœé–¢ä¿‚ãŒã‚ã‚‹ã®ã‹ä¸€è¨€èª¬æ˜
3. ã§ãã‚Œã°å†…å®¹ã®è¦ç´„ï¼ˆ1è¡Œï¼‰
4. å‚è€ƒã«ã—ãŸåŸæœ¬ï¼ˆæŠœç²‹orè¦ç´„å¯¾è±¡ï¼‰

ã‚‚ã—é–¢é€£ã™ã‚‹ãƒˆãƒ”ãƒƒã‚¯ãŒãªã„å ´åˆã¯ã€ã€Œã“ã®1é€±é–“ã€AIã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã«ç‰¹ã«é–¢é€£ã™ã‚‹å¤§ããªã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€ã¨å›ç­”ã—ã¦ãã ã•ã„ã€‚
"""
    return agent(prompt)

# === å®Ÿè¡Œéƒ¨åˆ† ===
if __name__ == "__main__":
    print("ğŸ” AWS What's New RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’åé›†ä¸­...")
    news = scrape_aws_news_rss()
    
    # RSSãƒ•ã‚£ãƒ¼ãƒ‰ãŒå¤±æ•—ã—ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    if not news:
        print("ğŸ”„ RSSãƒ•ã‚£ãƒ¼ãƒ‰ãŒå¤±æ•—ã—ãŸãŸã‚ã€AWS News Blogã‚’è©¦è¡Œä¸­...")
        news = scrape_aws_news()
    
    if news:
        print("ğŸ§  ClaudeãŒè¦ç´„ä¸­...")
        summary = summarize_with_claude(news)
        print("\n===== ä»Šé€±ã®ãƒ”ãƒƒã‚¯ã‚¢ãƒƒãƒ—ã¾ã¨ã‚ =====\n")
        print(summary)
        print("\n==================================\n")
    else:
        print("âŒ ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚µã‚¤ãƒˆã®æ§‹é€ ãŒå¤‰æ›´ã•ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
